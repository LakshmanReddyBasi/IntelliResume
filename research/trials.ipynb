{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae89c969",
   "metadata": {},
   "source": [
    "# ü§ñRAG ‚öïÔ∏èHealthbot Pipeline \n",
    "## Building a Retrieval-Augmented Generation (RAG) pipeline for a healthbot using LangChain, Pinecone, and Gemini.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b5333",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports\n",
    "Get current working directory and import document loaders and text splitter from LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1d73755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\LakshmanReddy\\\\OneDrive\\\\Documents\\\\AI-ML\\\\projects\\\\RAG PROJECT\\\\RAG PROJECT\\\\LLM-RAG-HEALTHBOT\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b207d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader , DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09741f8",
   "metadata": {},
   "source": [
    "## 2. PDF Extraction\n",
    "Define a function to extract documents from PDF files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd7b7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text from the pdf file\n",
    "\n",
    "def  extract_from_pdf(file_path):\n",
    "    loader = DirectoryLoader(file_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "902c1dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "C:\\Users\\LakshmanReddy\\AppData\\Local\\Temp\\ipykernel_19608\\2960162591.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  extracted_docs = extract_from_pdf(\"..\\medical data\")\n",
      "Error loading file ..\\medical data\\Medical_book.pdf\n",
      "C:\\Users\\LakshmanReddy\\AppData\\Local\\Temp\\ipykernel_19608\\2960162591.py:1: SyntaxWarning: invalid escape sequence '\\m'\n",
      "  extracted_docs = extract_from_pdf(\"..\\medical data\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "`pypdf` package not found, please install it with `pip install pypdf`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:359\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdf\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pypdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m extracted_docs \u001b[38;5;241m=\u001b[39m extract_from_pdf(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmedical data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m, in \u001b[0;36mextract_from_pdf\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m  \u001b[38;5;21mextract_from_pdf\u001b[39m(file_path):\n\u001b[0;32m      4\u001b[0m     loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(file_path, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m, loader_cls\u001b[38;5;241m=\u001b[39mPyPDFLoader)\n\u001b[1;32m----> 5\u001b[0m     documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m documents\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\directory.py:195\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m items:\n\u001b[1;32m--> 195\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_load_file(i, p, pbar)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pbar:\n\u001b[0;32m    198\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\directory.py:233\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[1;34m(self, item, path, pbar)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(item)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 233\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pbar:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\directory.py:223\u001b[0m, in \u001b[0;36mDirectoryLoader._lazy_load_file\u001b[1;34m(self, item, path, pbar)\u001b[0m\n\u001b[0;32m    221\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_cls(\u001b[38;5;28mstr\u001b[39m(item), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_kwargs)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdoc \u001b[38;5;129;01min\u001b[39;00m loader\u001b[38;5;241m.\u001b[39mlazy_load():\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m subdoc\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\pdf.py:305\u001b[0m, in \u001b[0;36mPyPDFLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mlazy_parse(blob)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:361\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpypdf\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pypdf` package not found, please install it with `pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_text_from_page\u001b[39m(page: pypdf\u001b[38;5;241m.\u001b[39mPageObject) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    366\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m    Extract text from image given the version of pypdf.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m        str: The extracted text.\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: `pypdf` package not found, please install it with `pip install pypdf`"
     ]
    }
   ],
   "source": [
    "extracted_docs = extract_from_pdf(\"..\\medical data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1d552",
   "metadata": {},
   "source": [
    "\n",
    "Display extracted documents and show number of extracted documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3bfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c7505",
   "metadata": {},
   "source": [
    "## 3. Document Preprocessing\n",
    "Import Document schema and define filter_to_minimal_docs to keep only source and page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def filter_to_minimal_docs(docs) :\n",
    "    \"\"\"\n",
    "    Given a list of Document objects, return a new list of Document objects\n",
    "    containing only 'source' in metadata and the original page_content.\n",
    "    \"\"\"\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(\n",
    "            Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata={\"source\": src}\n",
    "            )\n",
    "        )\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimal_docs = filter_to_minimal_docs(extracted_docs)\n",
    "minimal_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7317a1b",
   "metadata": {},
   "source": [
    "## 4. Text Chunking\n",
    "Define chunker function, chunk minimal documents, print number of chunks, and display first chunk‚Äôs content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98ff2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking the documents into smaller pieces\n",
    "\n",
    "def chunker(docs ,chunk_size=1200 , chunk_oerlap= 100):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_oerlap\n",
    "    )\n",
    "    text_chunks= text_splitter.split_documents(docs)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d321be",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = chunker(minimal_docs)\n",
    "print(f\"Number of chunks: {len(text_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1249ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e44975",
   "metadata": {},
   "source": [
    "## 5. Embedding Model Setup\n",
    "Import and initialize HuggingFace embeddings. Test embedding with a sample sentence. Display embedding vector size and sample values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ddcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_embeddings():\n",
    "    \"\"\"\n",
    "    Download and return the HuggingFace embeddings model.\n",
    "    \"\"\"\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52342c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vector = embedding.embed_query(\"This is an example sentence to be embedded.\")\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91de746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vector))                         # As its a 384 dimensional vector\n",
    "print(vector[:5])                          # Print first 5 dimensions of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec64a98",
   "metadata": {},
   "source": [
    "## 6. Environment Variables\n",
    "Load .env file and read Pinecone and Gemini API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv() \n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63e8f0",
   "metadata": {},
   "source": [
    "## 7. Pinecone Index Setup\n",
    "Initialize Pinecone client and create Pinecone index if not exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad0f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "# initialize pinecone client (pc)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bae0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "index_name=\"medical\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=384,        #dimesnions of the sentence transformer model\n",
    "        metric=\"cosine\",      #codine similarity\n",
    "        spec=ServerlessSpec(cloud=\"aws\",region=\"us-east-1\")\n",
    "        \n",
    "    )\n",
    "my_index=pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f8b66",
   "metadata": {},
   "source": [
    "## 8. Vector Store Creation\n",
    "Import and create PineconeVectorStore from document chunks and embeddings. Explain docsearch usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26266da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embedding\n",
    ")\n",
    "# docsearch is an instance of PineconeVectorStore, which is a LangChain wrapper for storing and searching document embeddings in a Pinecone index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd16d0",
   "metadata": {},
   "source": [
    "- add more data if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0629e1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = Document(\n",
    "    page_content=\"This is the page content of the new document .\",\n",
    "    metadata={\"source\": \"social media\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch.add_documents([new_doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5ae2c4",
   "metadata": {},
   "source": [
    "## 9. Retrieval\n",
    "- Create retriever from vector store. Retrieve data for a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a4c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_data = retriever.invoke(\"what is acne ?\")\n",
    "retrieved_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db96ae9e",
   "metadata": {},
   "source": [
    "## 10. LLM Setup\n",
    "- Import and initialize Gemini chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c17bcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2cbab",
   "metadata": {},
   "source": [
    "## 11. RAG Chain Construction\n",
    "- Import chain and prompt utilities. Define system and human prompts. Create document combination and retrieval chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42daaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74be9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an Medical assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. keep the answer concise .\"\n",
    "    \n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b388287",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(chat_model, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0800ca",
   "metadata": {},
   "source": [
    "## 12. End-to-End QA\n",
    "- Invoke RAG chain with a sample medical question. Print the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c48fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke({\"input\": \"what is Acromegaly and gigantism?\"})\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
